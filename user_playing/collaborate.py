from training import GridEnvironment, SelfPlayAgent
import numpy as np

def user_choose_action():
    key_map = {'w': 0, 's': 1, 'a': 2, 'd': 3}
    while True:
        key = input("ì‚¬ìš©ì ë°©í–¥ ì…ë ¥ (â†‘:w â†“:s â†:a â†’:d): ").strip().lower()
        if key in key_map:
            return key_map[key]
        else:
            print("â—ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. w/s/a/d ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


def cooperative_play(env, agent1, episodes=5):
    agent1.epsilon = 0  # í•™ìŠµëœ ì—ì´ì „íŠ¸ëŠ” í™œìš©ë§Œ í•¨

    for episode in range(episodes):
        env.reset()
        print(f"\nâ–¶ ì—í”¼ì†Œë“œ {episode+1} ì‹œì‘")
        print(f"ë³´ìƒ ìœ„ì¹˜: {env.reward_pos}")

        done = False
        step = 0

        while not done:
            print(f"\nğŸ§­ ìŠ¤í… {step}")
            print(f"ì—ì´ì „íŠ¸1 ìœ„ì¹˜: {env.agent1_pos}")
            print(f"ì—ì´ì „íŠ¸2 ìœ„ì¹˜: {env.agent2_pos}")

            # ì—ì´ì „íŠ¸1 ìë™ ì„ íƒ
            action1 = agent1.choose_action(env.agent1_pos)
            env.move(env.agent1_pos, action1)
            print(f"ì—ì´ì „íŠ¸1 ì´ë™ ë°©í–¥: {['â†‘','â†“','â†','â†’'][action1]}")

            # ì‚¬ìš©ì ì…ë ¥
            action2 = user_choose_action()
            env.move(env.agent2_pos, action2)

            # ê²°ê³¼ í™•ì¸
            if env.check_reward(env.agent1_pos) or env.check_reward(env.agent2_pos):
                print("ğŸ‰ ë³´ìƒ íšë“!")
                done = True
            elif env.check_collision():
                print("ğŸ’¥ ì¶©ëŒ ë°œìƒ!")
                done = True

            step += 1

        print(f"ì—í”¼ì†Œë“œ {episode+1} ì¢…ë£Œ.\n")


# ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸° (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ë„ ë¨)
env = GridEnvironment()
agent1 = SelfPlayAgent(env, agent_id=0)
agent1.q_table = np.load("agent1_q_table.npy")  # í•™ìŠµëœ Q-table ë¶ˆëŸ¬ì˜¤ê¸°

cooperative_play(env, agent1, episodes=3)
